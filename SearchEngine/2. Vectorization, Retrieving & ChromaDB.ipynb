{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6270fc7",
   "metadata": {},
   "source": [
    "# Continuing with the CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8007da1",
   "metadata": {},
   "source": [
    "# Step - 5: Experiment with Text Vectorization\n",
    "### Using Bag of Words (BOW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53253fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data from the CSV file\n",
    "cleaned_data = pd.read_csv('cleaned_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2111575a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\sampada\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\sampada\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.39.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sampada\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing the necessary libraries\n",
    "!pip install scikit-learn\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0246ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Initialize vectorizers\n",
    "bow_vectorizer = CountVectorizer()  # For Bag-of-Words (BoW)\n",
    "bow_vectors = bow_vectorizer.fit_transform(cleaned_data['cleaned_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f7b22",
   "metadata": {},
   "source": [
    "### Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24666202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vectorizers\n",
    "# For TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()  \n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(cleaned_data['cleaned_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09f605",
   "metadata": {},
   "source": [
    "### Using BERT based Sentence Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a99c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize vectorizers\n",
    "# For TF-IDF\n",
    "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "bert_vectors = bert_model.encode(cleaned_data['cleaned_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13eb82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to find similar records based on a query using BERT embeddings\n",
    "def find_similar_records_bert(query, df, bert_model, bert_vectors):\n",
    "    # Encode the query using the BERT model\n",
    "    query_vector = bert_model.encode([query])\n",
    "    \n",
    "    # Compute cosine similarities between the query vector and all document vectors\n",
    "    cosine_similarities = cosine_similarity(query_vector, bert_vectors).flatten()\n",
    "    \n",
    "    # Sort the indices based on cosine similarities in descending order\n",
    "    similar_records_indices = cosine_similarities.argsort()[::-1]\n",
    "    \n",
    "    # Get the similar records based on the sorted indices\n",
    "    similar_records = df.iloc[similar_records_indices]\n",
    "    \n",
    "    # Add a 'similarity' column to the DataFrame\n",
    "    similar_records['similarity'] = cosine_similarities[similar_records_indices]\n",
    "    \n",
    "    return similar_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f80b72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data from the CSV file\n",
    "cleaned_data = pd.read_csv('cleaned_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3110da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data from the CSV file\n",
    "cleaned_data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Initialize BERT model\n",
    "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Encode cleaned text using BERT model\n",
    "bert_vectors = bert_model.encode(cleaned_data['cleaned_text'])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ee6a1",
   "metadata": {},
   "source": [
    "# Step - 6: Data Retrieval \n",
    "### Keyboard-based Search Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840a5aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query: Action\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sampada\\AppData\\Local\\Temp\\ipykernel_15760\\3756439137.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_records['similarity'] = cosine_similarities[similar_records_indices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of search results you want: 10\n",
      "Top 10 search results for query 'Action':\n",
      "                                                    name  similarity\n",
      "47353  il.grande.gioco.s01.e04.contropiede.(2022).eng...    0.436760\n",
      "80375  tooth.pari.when.love.bites.s01.e03.episode.1.3...    0.407746\n",
      "80380  tooth.pari.when.love.bites.s01.e08.episode.1.8...    0.401364\n",
      "69691  shadow.and.bone.s02.e08.no.funerals.(2023).eng...    0.399929\n",
      "31258   warrior.nun.s02.e08.jeremiah.2913.(2022).eng.1cd    0.398344\n",
      "16902  thai.cave.rescue.s01.e05.the.parable.of.kisa.g...    0.384536\n",
      "50266  vikings.valhalla.s02.e02.towers.of.faith.(2023...    0.383630\n",
      "69685  shadow.and.bone.s02.e01.no.shelter.but.me.(202...    0.379811\n",
      "75811  beef.s01.e06.we.draw.a.magic.circle.(2023).eng...    0.375271\n",
      "56683           class.s01.e07.episode.1.7.(2023).eng.1cd    0.369109\n"
     ]
    }
   ],
   "source": [
    "# Define a function to prompt the user to input the query\n",
    "def get_user_query():\n",
    "    return input(\"Enter query: \")\n",
    "\n",
    "# Prompt the user to input the query\n",
    "query = get_user_query()\n",
    "\n",
    "# Find similar records based on the query using BERT embeddings\n",
    "search_results_bert = find_similar_records_bert(query, cleaned_data, bert_model, bert_vectors)\n",
    "\n",
    "# Prompt the user to input the number of search results they want\n",
    "top_n = int(input('Enter the number of search results you want: '))\n",
    "\n",
    "# Display the top N search results for the query\n",
    "print(f\"Top {top_n} search results for query '{query}':\")\n",
    "print(search_results_bert[['name', 'similarity']].head(top_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42146e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to find similar records based on a query using BERT embeddings\n",
    "def find_similar_records_semantic(query, cleaned_data, bert_model, bert_vectors):\n",
    "    # Encode the query using BERT model\n",
    "    query_vector = bert_model.encode([query])[0]\n",
    "    \n",
    "    # Calculate cosine similarity between query vector and all document vectors\n",
    "    similarity_scores = cosine_similarity([query_vector], bert_vectors)[0]\n",
    "    \n",
    "    # Sort indices based on similarity scores\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "    \n",
    "    # Get similar records\n",
    "    similar_records = cleaned_data.iloc[sorted_indices]\n",
    "    \n",
    "    return similar_records, similarity_scores[sorted_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fc9b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "query = \"Action\"\n",
    "search_results_semantic, similarity_scores = find_similar_records_semantic(query, cleaned_data, bert_model, bert_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cac791",
   "metadata": {},
   "source": [
    "### Semantic-based Search Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6738220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find similar records based on a query using BERT embeddings\n",
    "def find_similar_records_bert(query, df, bert_model, bert_vectors):\n",
    "    # Encode the query using BERT model\n",
    "    query_vector = bert_model.encode([query])\n",
    "    \n",
    "    # Compute cosine similarity between the query vector and all document vectors\n",
    "    similarities = cosine_similarity(query_vector, bert_vectors)[0]\n",
    "    \n",
    "    # Sort the indices based on similarity scores in descending order\n",
    "    indices = similarities.argsort()[::-1]\n",
    "    \n",
    "    # Extract the top similar records\n",
    "    similar_records = df.iloc[indices]\n",
    "    \n",
    "    return similar_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88538598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query: Action\n",
      "Enter the number of search results you want: 10\n",
      "Top 10 search results for query 'Action':\n",
      "47353    il.grande.gioco.s01.e04.contropiede.(2022).eng...\n",
      "80375    tooth.pari.when.love.bites.s01.e03.episode.1.3...\n",
      "80380    tooth.pari.when.love.bites.s01.e08.episode.1.8...\n",
      "69691    shadow.and.bone.s02.e08.no.funerals.(2023).eng...\n",
      "31258     warrior.nun.s02.e08.jeremiah.2913.(2022).eng.1cd\n",
      "16902    thai.cave.rescue.s01.e05.the.parable.of.kisa.g...\n",
      "50266    vikings.valhalla.s02.e02.towers.of.faith.(2023...\n",
      "69685    shadow.and.bone.s02.e01.no.shelter.but.me.(202...\n",
      "75811    beef.s01.e06.we.draw.a.magic.circle.(2023).eng...\n",
      "56683             class.s01.e07.episode.1.7.(2023).eng.1cd\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define a function to prompt the user to input the query\n",
    "def get_user_query():\n",
    "    return input(\"Enter query: \")\n",
    "\n",
    "# Get user input for the query\n",
    "query = get_user_query()\n",
    "\n",
    "# Find similar records based on the query using BERT embeddings\n",
    "search_results_bert = find_similar_records_bert(query, cleaned_data, bert_model, bert_vectors)\n",
    "\n",
    "# Prompt the user to input the number of search results they want\n",
    "top_n = int(input('Enter the number of search results you want: '))\n",
    "\n",
    "# Display the top N search results for the query\n",
    "print(f\"Top {top_n} search results for query '{query}':\")\n",
    "print(search_results_bert['name'].head(top_n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e824b6",
   "metadata": {},
   "source": [
    "# Step - 7: Embedding to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbf120bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9511fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a my_chromadb folder in the work directory\n",
    "chroma_client = chromadb.PersistentClient(path=\"my_chromadb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99563047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the distilbert-base-nli-mean-tokens model for embedding function\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"distilbert-base-nli-mean-tokens\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\", embedding_function=sentence_transformer_ef, metadata={\"hnsw:space\": \"cosine\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf17c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads your DataFrame directly into memory\n",
    "se_emd = pd.read_csv('cleaned_data.csv' ,nrows = 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93eeec90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Adds documents from DataFrame along with metadata and IDs to the collection\n",
    "collection.add(\n",
    "    documents=se_emd['name'].tolist(),\n",
    "    metadatas=[{\"item_id\": str(idx)} for idx in range(len(se_emd))],\n",
    "    ids=[str(idx) for idx in range(len(se_emd))],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fe0a7",
   "metadata": {},
   "source": [
    "# Step - 8: Retrieving Based on Search/Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "986f2822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: Action\n",
      "Your search query: Action\n",
      " *** allegoria.().eng.1cd *** \n",
      " *** allegoria.().eng.1cd *** \n",
      " *** allegoria.().eng.1cd *** \n",
      " *** we.are.gathered.here.today.().eng.1cd *** \n",
      " *** we.are.gathered.here.today.().eng.1cd *** \n",
      " *** epoch.(2001).eng.1cd *** \n",
      " *** the.message.(1976).eng.1cd *** \n",
      " *** event.horizon.(1997).eng.1cd *** \n",
      " *** event.horizon.(1997).eng.1cd *** \n",
      " *** event.horizon.(1997).eng.1cd *** \n"
     ]
    }
   ],
   "source": [
    "# Getting user input\n",
    "user_query = input(\"Enter your search query: \")\n",
    "\n",
    "# Querying the collection\n",
    "results = collection.query(\n",
    "    query_texts=[user_query],\n",
    "    n_results=10,\n",
    "    include=['documents', 'distances', 'metadatas']\n",
    ")\n",
    "\n",
    "# Displaying the user input\n",
    "print(f\"Your search query: {user_query}\")\n",
    "\n",
    "# Displaying output documents\n",
    "for document in results['documents'][0]:\n",
    "    print(f\" *** {document} *** \")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
